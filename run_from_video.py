import os
import cv2
import torch
import argparse
import numpy as np
from tqdm import tqdm
from PIL import Image
from pathlib import Path
from ultralytics import YOLO
from collections import defaultdict
from typing import List, Tuple, Optional, Dict
import torchvision.transforms.functional as TF
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload
from google.oauth2.credentials import Credentials
import sys


from src.modeling.sharingan import Sharingan
from src.utils.common import spatial_argmax2d, square_bbox


DET_THR = 0.2  # head detection threshold
CONF_THR = 0.25
IOU_THR = 0.45
NUM_CLASSES = 6  # 0~5ë§Œ ì‚¬ìš©(í•„ìš” ì—†ìœ¼ë©´ None)

# CKPT_PATH = "checkpoints/best_gaze_synth_experiment_3.ckpt" # Sharingan ckpt ê²½ë¡œ
# OBJ_MODEL_PATH = "weights/detect_experiment_object.pt"    # ë¬¼ì²´ YOLO ê°€ì¤‘ì¹˜
# HEAD_MODEL_PATH = "weights/experiment_head.pt"
# VIDEO_PATH = "./data/20250630_160223.mp4"


IMG_MEAN = [0.461037, 0.486372, 0.471720]
IMG_STD = [0.212238, 0.208506, 0.205668]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

COLOR_OBJ = (0, 200, 0)  # ì´ˆë¡
COLOR_HIT = (0, 255, 255)  # ë…¸ë‘
COLOR_HEAD = (255, 180, 80)  # í•˜ëŠ˜/ì£¼í™©í†¤
COLOR_GAZE = (0, 0, 255)  # ë¹¨ê°•

# run_from_video.py ë§¨ ìœ„ (importë“¤ ë°‘)
OUT_DIR = Path("out")
OUT_DIR.mkdir(exist_ok=True)


# ========================= UTILITY FUNCTIONS =========================== #
def resolve_path(p: str | Path) -> Path:
    """
    Pathë§Œ ì‚¬ìš©í•´ ê²½ë¡œë¥¼ ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜.
    - ì ˆëŒ€ê²½ë¡œë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
    - ìƒëŒ€ê²½ë¡œë©´ í˜„ì¬ ì‘ì—… ë””ë ‰í„°ë¦¬(Path.cwd()) ê¸°ì¤€ìœ¼ë¡œ ì ˆëŒ€í™”
    """
    p = Path(p).expanduser()
    return p if p.is_absolute() else (Path.cwd() / p).resolve()


def expand_bbox(bbox, img_w, img_h, k=0.1):
    w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]
    bbox[0] = max(0, bbox[0] - k * w)
    bbox[1] = max(0, bbox[1] - k * h)
    bbox[2] = min(img_w, bbox[2] + k * w)
    bbox[3] = min(img_h, bbox[3] + k * h)
    return bbox


def expand_xyxy_rel_img(
    xyxy: np.ndarray, img_w: int, img_h: int, kx: float = 0.02, ky: float = 0.02
) -> np.ndarray:
    """
    xyxy: (N,4) [x1,y1,x2,y2] float32
    img_w, img_h: ì´ë¯¸ì§€ í¬ê¸°
    kx, ky: ì´ë¯¸ì§€ í­/ë†’ì´ì— ëŒ€í•œ ë¹„ìœ¨ë¡œ í™•ì¥ (ì˜ˆ: 0.02 -> ì–‘ìª½ìœ¼ë¡œ ê°ê° ì´ë¯¸ì§€ì˜ 2%ì”©)
    """
    if xyxy.size == 0:
        return xyxy
    dx = kx * img_w
    dy = ky * img_h
    out = xyxy.copy()
    out[:, 0] = np.maximum(0.0, out[:, 0] - dx)
    out[:, 1] = np.maximum(0.0, out[:, 1] - dy)
    out[:, 2] = np.minimum(img_w - 1.0, out[:, 2] + dx)
    out[:, 3] = np.minimum(img_h - 1.0, out[:, 3] + dy)
    return out.astype(np.float32)


def load_obj_detection_model(device, weights_path: Optional[str] = None):
    model = YOLO(weights_path)
    model = model.to(device)
    model.eval()
    return model


def load_head_detection_model(device, weights_path: Optional[str] = None):
    model = YOLO(weights_path)
    model = model.to(device)
    model.eval()
    return model


def detect_heads(image, model):
    image_cv = np.array(image)[..., ::-1]
    results = model(
        image_cv, imgsz=640, conf=0.25, iou=0.45, classes=[0], amp=False, verbose=False
    )
    boxes = results[0].boxes.xyxy.cpu().numpy()  # (N, 4)
    confs = results[0].boxes.conf.cpu().numpy()  # (N,)
    detections = np.hstack([boxes, confs[:, None]])  # (N, 5)
    return detections


def load_sharingan_model(ckpt_path, device):
    sharingan = Sharingan(
        patch_size=16,
        token_dim=768,
        image_size=224,
        gaze_feature_dim=512,
        encoder_depth=12,
        encoder_num_heads=12,
        encoder_num_global_tokens=0,
        encoder_mlp_ratio=4.0,
        encoder_use_qkv_bias=True,
        encoder_drop_rate=0.0,
        encoder_attn_drop_rate=0.0,
        encoder_drop_path_rate=0.0,
        decoder_feature_dim=128,
        decoder_hooks=[2, 5, 8, 11],
        decoder_hidden_dims=[48, 96, 192, 384],
        decoder_use_bn=True,
    )
    checkpoint = torch.load(ckpt_path, map_location="cpu", weights_only=False)
    checkpoint = {
        name.replace("model.", ""): value
        for name, value in checkpoint["state_dict"].items()
    }
    sharingan.load_state_dict(checkpoint, strict=True)
    sharingan.eval()
    sharingan.to(device)
    return sharingan


def rank_interest_scores(
    interest: Dict[int, int],
    class_names: Optional[List[str]] = None,
    top_k: Optional[int] = None,
) -> List[Tuple[int, int, str]]:
    """
    ê´€ì‹¬ë„ dictë¥¼ ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ [(cls_id, score, name)] ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    ranked = sorted(interest.items(), key=lambda kv: (-kv[1], kv[0]))
    if top_k is not None:
        ranked = ranked[:top_k]
    out = []
    for cid, score in ranked:
        name = (
            class_names[cid] if (class_names and 0 <= cid < len(class_names)) else "-"
        )
        out.append((cid, score, name))
    return out


def predict_gaze(image, sharingan, head_detector, tracker=None):
    # 1. Convert image
    image_np = np.array(image)
    img_h, img_w, _ = image_np.shape

    raw_detections = detect_heads(image_np, head_detector)
    detections = []
    for raw in raw_detections:
        bbox, conf = raw[:4], raw[4]
        if conf > DET_THR:
            bbox = expand_bbox(bbox, img_w, img_h, k=0.2)
            cls_ = np.array([0.0])
            detection = np.concatenate(
                [bbox, conf[None], cls_]
            )  # [x1,y1,x2,y2,conf,class_id]
            detections.append(detection)

    if len(detections) == 0:
        return (
            torch.tensor([]),
        ) * 6  # gaze_points, gaze_vecs, inouts, head_bboxes, gaze_heatmaps, pids

    detections = np.stack(detections)
    if tracker is not None:
        tracks = tracker.update(detections, image_np)
        if len(tracks) == 0:
            return (torch.tensor([]),) * 6
        pids = (tracks[:, 4] - 1).astype(int)
        head_bboxes = torch.from_numpy(tracks[:, :4]).float()
    else:
        head_bboxes = torch.from_numpy(detections[:, :4]).float()
        pids = np.arange(len(head_bboxes))

    t_head_bboxes = square_bbox(head_bboxes, img_w, img_h)

    # 3. Extract and transform heads
    heads = []
    for bbox in t_head_bboxes:
        head = TF.resize(
            TF.to_tensor(image.crop(bbox.numpy())), (224, 224), antialias=True
        )
        heads.append(head)
    heads = torch.stack(heads)
    heads = TF.normalize(heads, mean=IMG_MEAN, std=IMG_STD)

    # 4. Transform Image
    image = TF.to_tensor(image)
    image = TF.resize(image, (224, 224), antialias=True)
    image = TF.normalize(image, mean=IMG_MEAN, std=IMG_STD)

    # 5. Normalize head bboxes
    scale = torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)
    t_head_bboxes /= scale

    # 6. build input sample
    sample = {
        "image": image.unsqueeze(0).to(DEVICE),
        "heads": heads.unsqueeze(0).to(DEVICE),
        "head_bboxes": t_head_bboxes.unsqueeze(0).to(DEVICE),
    }

    # 7. predict gaze
    with torch.no_grad():
        gaze_vecs, gaze_heatmaps, inouts = sharingan(sample)
        gaze_heatmaps = gaze_heatmaps.squeeze(0).cpu()
        gaze_vecs = gaze_vecs.squeeze(0).cpu()
        gaze_points = spatial_argmax2d(gaze_heatmaps, normalize=True)
        inouts = torch.sigmoid(inouts.squeeze(0)).flatten().cpu()

    return gaze_points, gaze_vecs, inouts, head_bboxes, gaze_heatmaps, pids


def point_in_any_box(pt: Tuple[float, float], boxes_xyxy: np.ndarray):
    """í¬ì¸íŠ¸ê°€ í¬í•¨ëœ ë°•ìŠ¤ ì¸ë±ìŠ¤(ì—¬ëŸ¿ì´ë©´ ë” ì‘ì€ ë°•ìŠ¤ ìš°ì„ ). ì—†ìœ¼ë©´ None."""
    if boxes_xyxy.size == 0:
        return None
    x, y = pt
    inside = (
        (boxes_xyxy[:, 0] <= x)
        & (x <= boxes_xyxy[:, 2])
        & (boxes_xyxy[:, 1] <= y)
        & (y <= boxes_xyxy[:, 3])
    )
    idxs = np.where(inside)[0]
    if len(idxs) == 0:
        return None
    areas = (boxes_xyxy[idxs, 2] - boxes_xyxy[idxs, 0]) * (
        boxes_xyxy[idxs, 3] - boxes_xyxy[idxs, 1]
    )
    return idxs[np.argmin(areas)]


# ========================= ë¹„ë””ì˜¤ ì²˜ë¦¬ ìœ í‹¸ ========================= #


def safe_open_video_writer(
    out_path: str, fps: float, size: tuple[int, int]
) -> cv2.VideoWriter:
    W, H = size
    ext = Path(out_path).suffix.lower()

    # 1) ìš°ì„  ì»¨í…Œì´ë„ˆì— ë§ëŠ” ì½”ë± ì‹œë„
    if ext == ".mp4":
        candidates = [
            ("mp4v", ".mp4"),  # ê°€ì¥ ë¬´ë‚œ
            ("avc1", ".mp4"),  # H.264 (ì¸ì½”ë” í•„ìš”)
            ("H264", ".mp4"),  # ê°™ì€ ê³„ì—´
        ]
    else:
        candidates = [
            ("XVID", ".avi"),
            ("MJPG", ".avi"),
        ]

    for fourcc_tag, new_ext in candidates:
        # í™•ì¥ì ë³´ì •
        path_try = str(Path(out_path).with_suffix(new_ext))
        fourcc = cv2.VideoWriter_fourcc(*fourcc_tag)
        vw = cv2.VideoWriter(path_try, fourcc, max(fps, 1.0), (W, H))
        if vw.isOpened():
            if path_try != out_path:
                print(
                    f"[INFO] changed output to {path_try} (codec={fourcc_tag})",
                    file=sys.stderr,
                )
            return vw

    for fourcc_tag in ["XVID", "MJPG"]:
        path_try = str(Path(out_path).with_suffix(".avi"))
        fourcc = cv2.VideoWriter_fourcc(*fourcc_tag)
        vw = cv2.VideoWriter(path_try, fourcc, max(fps, 1.0), (W, H))
        if vw.isOpened():
            print(
                f"[INFO] fallback to {path_try} (codec={fourcc_tag})", file=sys.stderr
            )
            return vw

    raise RuntimeError(
        "Failed to open VideoWriter with common codecs. "
        "Install FFmpeg with x264/x265 or use AVI/MJPG."
    )


def run_object_detector_on_frame(
    model: YOLO,
    frame_bgr: np.ndarray,
    conf: float = CONF_THR,
    iou: float = IOU_THR,
    num_classes: Optional[int] = NUM_CLASSES,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    ë‹¨ì¼ í”„ë ˆì„ì—ì„œ ë¬¼ì²´ íƒì§€.
    Returns:
        xyxy: (N,4) float32
        cls:  (N,)  int64
    """
    if frame_bgr is None or frame_bgr.size == 0:
        raise RuntimeError("frame_bgr cannot be None")

    # UltralyticsëŠ” ndarrayë„ ì…ë ¥ ê°€ëŠ¥ (BGR OK)
    res = model.predict(frame_bgr, conf=conf, iou=iou, verbose=False)[0]
    if res.boxes is None or len(res.boxes) == 0:
        return np.zeros((0, 4), np.float32), np.zeros((0,), np.int64)

    xyxy = res.boxes.xyxy.cpu().numpy().astype(np.float32)
    cls = res.boxes.cls.cpu().numpy().astype(np.int64)
    if num_classes is not None:
        keep = (cls >= 0) & (cls < num_classes)
        xyxy, cls = xyxy[keep], cls[keep]
    return xyxy, cls


def get_pred_gaze_points_pixels_from_frame(
    frame_bgr: np.ndarray,
    sharingan,
    head_detector,
    filter_by_inout: bool = False,
    io_thr: float = 0.5,
) -> Tuple[np.ndarray, int]:
    """
    í”„ë ˆì„ 1ì¥ì— ëŒ€í•´ Sharinganìœ¼ë¡œ ì˜ˆì¸¡í•œ ëª¨ë“  ì‹œì„  ì ì„ 'í”½ì…€ ì¢Œí‘œ'ë¡œ ë°˜í™˜.
    Returns:
        pts_pix: (M, 2) float32, ê° headì˜ (x,y) in pixels
        num_heads: ê°ì§€ëœ head ìˆ˜
    """
    if frame_bgr is None:
        raise ValueError("frame_bgr cannot be None")

    H, W = frame_bgr.shape[:2]
    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    img_pil = Image.fromarray(img_rgb)

    out = predict_gaze(img_pil, sharingan, head_detector, tracker=None)
    gaze_points, gaze_vecs, inouts, head_bboxes, gaze_heatmaps, pids = out

    if gaze_points is None or len(gaze_points) == 0:
        return np.zeros((0, 2), dtype=np.float32), 0

    gp = (
        gaze_points.cpu().numpy()
        if hasattr(gaze_points, "cpu")
        else np.array(gaze_points)
    )
    pts_pix = gp * np.array([W, H], dtype=np.float32)

    if filter_by_inout and (inouts is not None) and (len(inouts) == len(pts_pix)):
        io = inouts.cpu().numpy() if hasattr(inouts, "cpu") else np.array(inouts)
        keep = io > io_thr
        pts_pix = pts_pix[keep]

    return pts_pix.astype(np.float32), (0 if head_bboxes is None else len(head_bboxes))


def get_pred_gaze_points_and_heads_from_frame(
    frame_bgr: np.ndarray,
    sharingan,
    head_detector,
    filter_by_inout: bool = False,
    io_thr: float = 0.5,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Returns:
        gaze_pts_pix: (K,2) float32  - í”½ì…€ ì¢Œí‘œ (x,y)
        head_xyxy:    (M,4) int32    - ë¨¸ë¦¬ ë°•ìŠ¤ (í”½ì…€)
    """
    H, W = frame_bgr.shape[:2]
    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)
    img_pil = Image.fromarray(img_rgb)

    gaze_points, gaze_vecs, inouts, head_bboxes, gaze_heatmaps, pids = predict_gaze(
        img_pil, sharingan, head_detector, tracker=None
    )

    # (1) gaze í”½ì…€ ì¢Œí‘œ
    if gaze_points is None or len(gaze_points) == 0:
        gaze_pts_pix = np.zeros((0, 2), dtype=np.float32)
    else:
        gp = (
            gaze_points.cpu().numpy()
            if hasattr(gaze_points, "cpu")
            else np.array(gaze_points)
        )
        gaze_pts_pix = gp * np.array([W, H], dtype=np.float32)
        if (
            filter_by_inout
            and (inouts is not None)
            and (len(inouts) == len(gaze_pts_pix))
        ):
            io = inouts.cpu().numpy() if hasattr(inouts, "cpu") else np.array(inouts)
            gaze_pts_pix = gaze_pts_pix[io > io_thr]

    # (2) head ë°•ìŠ¤ (í”½ì…€)
    if head_bboxes is None or (
        hasattr(head_bboxes, "numel") and head_bboxes.numel() == 0
    ):
        head_xyxy = np.zeros((0, 4), dtype=np.int32)
    else:
        if hasattr(head_bboxes, "cpu"):
            head_xyxy = head_bboxes.cpu().numpy()
        else:
            head_xyxy = np.array(head_bboxes)
        head_xyxy = head_xyxy.astype(np.int32)

    return gaze_pts_pix.astype(np.float32), head_xyxy


def accumulate_interest_from_video(
    video_path: str,
    obj_model: YOLO,
    head_model: YOLO,
    sharingan: Sharingan,
    expand_kx: float = 0.01,
    expand_ky: float = 0.01,
    frame_stride: int = 1,  # ë§¤ í”„ë ˆì„ì´ë©´ 1, 2ë©´ 1/2 ìƒ˜í”Œë§
    max_frames: Optional[int] = None,
    filter_by_inout: bool = False,
    io_thr: float = 0.5,
) -> Tuple[Dict[int, int], Dict[str, int]]:
    """
    ë™ì˜ìƒì„ ì²˜ìŒë¶€í„° ëê¹Œì§€ ëŒë©°:
      - obj_modelë¡œ í”„ë ˆì„ì˜ ë¬¼ì²´ íƒì§€ (ë°•ìŠ¤ í™•ì¥)
      - head_model + sharinganìœ¼ë¡œ ëª¨ë“  headì˜ gaze í”½ì…€ ì¢Œí‘œ ì¶”ì •
      - ê° gaze ì ì´ í¬í•¨ëœ ë¬¼ì²´ ë°•ìŠ¤ì˜ 'í´ë˜ìŠ¤' ê´€ì‹¬ë„ +1
    Returns:
        interest: {class_id: score}
        stats: {'n_frames':X, 'n_proc_frames':Y, 'n_heads':Z, 'n_hits':K}
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    interest: Dict[int, int] = defaultdict(int)
    if cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0:
        n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    else:
        raise RuntimeError(f"Frame count is zero")
    n_proc = 0
    n_heads = 0
    n_hits = 0

    processed_frames = 0
    frame_idx = 0

    # ë¹„ë””ì˜¤ ì´ í”„ë ˆì„ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    for _ in tqdm(range(total_frames), file=sys.stderr):
        ok, frame_bgr = cap.read()
        if not ok:
            break
        frame_idx += 1

        # í”„ë ˆì„ ìŠ¤í‚µ(ìƒ˜í”Œë§)
        if frame_stride > 1 and (frame_idx % frame_stride != 1):
            continue

        H, W = frame_bgr.shape[:2]

        # (1) obj detect
        obj_xyxy, obj_cls = run_object_detector_on_frame(
            obj_model, frame_bgr, conf=CONF_THR, iou=IOU_THR, num_classes=NUM_CLASSES
        )
        if obj_xyxy.size > 0:
            obj_xyxy = expand_xyxy_rel_img(obj_xyxy, W, H, kx=expand_kx, ky=expand_ky)

        # (2) gaze predict
        pts_pix, heads_here = get_pred_gaze_points_pixels_from_frame(
            frame_bgr=frame_bgr,
            sharingan=sharingan,
            head_detector=head_model,
            filter_by_inout=filter_by_inout,
            io_thr=io_thr,
        )

        n_proc += 1
        n_heads += int(heads_here)

        # (3) ê´€ì‹¬ë„ ëˆ„ì 
        if obj_xyxy.size > 0 and pts_pix.size > 0:
            for pt in pts_pix:
                idx = point_in_any_box((float(pt[0]), float(pt[1])), obj_xyxy)
                if idx is None:
                    continue
                cls_id = int(obj_cls[idx])
                if (NUM_CLASSES is None) or (0 <= cls_id < NUM_CLASSES):
                    interest[cls_id] += 1
                    n_hits += 1

        processed_frames += 1
        if (max_frames is not None) and (processed_frames >= max_frames):
            break

    cap.release()

    stats = {
        "n_frames": total_frames,
        "n_proc_frames": n_proc,
        "n_heads": n_heads,
        "n_hits": n_hits,
    }
    return dict(interest), stats


# ë©”íƒ€/ë­í‚¹/ì €ì¥ ìœ í‹¸


def write_ranking_txt(
    out_path: str,
    ranked: List[Tuple[int, int, str]],  # [(cls_id, score, name), ...]
    class_meta: Optional[Dict[int, Dict[str, str]]] = None,
    default_name_from_id: bool = True,
    add_unranked_from_meta: bool = True,  # â† ì¶”ê°€: metaì—ë§Œ ìˆëŠ” í´ë˜ìŠ¤ë„ ì¶œë ¥
    unranked_sort: str = "id",  # "id" ë˜ëŠ” "name"
) -> None:
    """
    í¬ë§·: "ìˆœìœ„, ì œí’ˆëª…, ì¹´í…Œê³ ë¦¬, ê°€ê²©"
    - ranked í•­ëª©: 1ë¶€í„° ìˆ«ì ìˆœìœ„ ë¶€ì—¬
    - class_metaì—ëŠ” ìˆìœ¼ë‚˜ rankedì—” ì—†ëŠ” í•­ëª©: ìˆœìœ„ "-"ë¡œ ì¶œë ¥ (ìˆ«ì ìˆœìœ„ ë’¤ì— ì´ì–´ì„œ)
    """
    lines = []

    # 1) ranked ë¨¼ì € (ìˆ«ì ìˆœìœ„)
    present_cids = set()
    for rank, (cid, score, name) in enumerate(ranked, start=1):
        present_cids.add(cid)
        if class_meta and cid in class_meta:
            pname = class_meta[cid].get(
                "name", name if default_name_from_id else str(cid)
            )
            cat = class_meta[cid].get("category", "-")
            price = class_meta[cid].get("price", "-")
        else:
            pname = name if default_name_from_id else str(cid)
            cat = "-"
            price = "-"
        lines.append(f"{rank},{pname},{cat},{price},{score}")

    # 2) metaì—ë§Œ ìˆëŠ” ë‚˜ë¨¸ì§€ (ìˆœìœ„ "-")
    if add_unranked_from_meta and class_meta:
        # ëˆ„ë½ëœ cid ìˆ˜ì§‘
        missing = [cid for cid in class_meta.keys() if cid not in present_cids]

        # ì •ë ¬ ë°©ì‹ ì„ íƒ
        if unranked_sort == "name":
            missing.sort(key=lambda cid: class_meta[cid].get("name", ""))
        else:  # "id"
            missing.sort()

        for cid in missing:
            rec = class_meta[cid]
            pname = rec.get("name", str(cid) if default_name_from_id else str(cid))
            cat = rec.get("category", "-")
            price = rec.get("price", "-")
            lines.append(f"-, {pname}, {cat}, {price},0")

    # ì €ì¥
    out_dir = str(Path(out_path).parent)
    if out_dir and out_dir not in (".", ""):
        Path(out_dir).mkdir(parents=True, exist_ok=True)

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

    print(f"[SAVE] ranking saved to: {out_path}", file=sys.stderr)


import os
from pathlib import Path

# --- ê²½ë¡œ ìë™ ê°ì§€ (ë¡œì»¬ or Render) ---
CREDENTIALS_PATH = (
    "/etc/secrets/credentials.json"
    if os.path.exists("/etc/secrets/credentials.json")
    else "credentials.json"
)

TOKENS_PATH = (
    "/etc/secrets/drive_token.json"
    if os.path.exists("/etc/secrets/drive_token.json")
    else "tokens/drive_token.json"
)


def upload_to_drive(file_path, mime_type="video/mp4", folder_id=None):
    """
    Google Driveì— íŒŒì¼ ì—…ë¡œë“œ í›„ (webContentLink, webViewLink) ë°˜í™˜
    """
    scopes = ["https://www.googleapis.com/auth/drive.file"]

    # âœ… ê²½ë¡œ ìˆ˜ì •: Render í™˜ê²½ì—ì„œëŠ” /etc/secrets/drive_token.json ì‚¬ìš©
    creds = Credentials.from_authorized_user_file(TOKENS_PATH, scopes)
    service = build("drive", "v3", credentials=creds)

    file_metadata = {"name": Path(file_path).name}
    if folder_id:
        file_metadata["parents"] = [folder_id]

    media = MediaFileUpload(file_path, mimetype=mime_type, resumable=True)
    uploaded = (
        service.files()
        .create(
            body=file_metadata,
            media_body=media,
            fields="id, webContentLink, webViewLink",
        )
        .execute()
    )

    return uploaded["webContentLink"], uploaded["webViewLink"]


# ========================= ë¹„ë””ì˜¤ ì‹œê°í™” & ì €ì¥ =========================


def _put_filled_text(
    img, text, org, fs=0.5, color=(255, 255, 255), bg=(0, 0, 0), thickness=1, pad=3
):
    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fs, thickness)
    x, y = org
    cv2.rectangle(img, (x - pad, y - th - pad), (x + tw + pad, y + pad), bg, -1)
    cv2.putText(
        img, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, fs, color, thickness, cv2.LINE_AA
    )


def build_filename(date, start, end):
    # "18:00" â†’ "1800"
    start_sanitized = start.replace(":", "")
    end_sanitized = end.replace(":", "")
    return f"{date}_{start_sanitized}~{end_sanitized}"


def draw_annotated_frame(
    frame_bgr: np.ndarray,
    obj_xyxy: np.ndarray,  # (N,4) float32  (í™•ì¥ ì ìš© í›„)
    obj_cls: Optional[np.ndarray],  # (N,)  int64
    head_xyxy: np.ndarray,  # (M,4) int32
    gaze_pts: np.ndarray,  # (K,2) float32
    class_names: Optional[List[str]] = None,
    thickness_obj: int = 2,
    thickness_head: int = 2,
    gaze_radius: int = 4,
) -> np.ndarray:
    """
    - ê°ì²´ ë°•ìŠ¤: ê¸°ë³¸ 'ì´ˆë¡' / ì‹œì„  ì ì´ ë“¤ì–´ê°„ ë°•ìŠ¤ëŠ” 'ë…¸ë‘'ìœ¼ë¡œ ë®ì–´ ê·¸ë¦¼
    - ë¨¸ë¦¬ ë°•ìŠ¤: í•˜ëŠ˜ìƒ‰
    - ì‹œì„  ì : ë¹¨ê°• ì›
    """
    vis = frame_bgr.copy()

    # ì–´ë–¤ obj ë°•ìŠ¤ê°€ í•˜ë‚˜ë¼ë„ ì‹œì„ ì„ í¬í•¨í–ˆëŠ”ì§€ ê³„ì‚°
    hit_mask = (
        np.zeros(len(obj_xyxy), dtype=bool)
        if obj_xyxy.size > 0
        else np.zeros((0,), dtype=bool)
    )
    if obj_xyxy.size > 0 and gaze_pts.size > 0:
        gx = gaze_pts[:, 0][:, None]
        gy = gaze_pts[:, 1][:, None]
        inside = (
            (gx >= obj_xyxy[:, 0])
            & (gx <= obj_xyxy[:, 2])
            & (gy >= obj_xyxy[:, 1])
            & (gy <= obj_xyxy[:, 3])
        )
        hit_mask = inside.any(axis=0)

    # 1) ê°ì²´ ë°•ìŠ¤
    for i, box in enumerate(obj_xyxy):
        x1, y1, x2, y2 = map(int, box)
        color = COLOR_HIT if (i < len(hit_mask) and hit_mask[i]) else COLOR_OBJ
        thick = max(thickness_obj, 3) if color == COLOR_HIT else thickness_obj
        cv2.rectangle(vis, (x1, y1), (x2, y2), color, thick)
        if obj_cls is not None and i < len(obj_cls):
            cid = int(obj_cls[i])
            label = f"{cid}"
            if class_names and (0 <= cid < len(class_names)):
                label = class_names[cid]
            _put_filled_text(
                vis, label, (x1 + 3, max(15, y1 - 4)), fs=0.5, bg=(30, 30, 30)
            )

    # 2) ë¨¸ë¦¬ ë°•ìŠ¤
    for hb in head_xyxy:
        x1, y1, x2, y2 = map(int, hb)
        cv2.rectangle(vis, (x1, y1), (x2, y2), COLOR_HEAD, thickness_head)

    # 3) ì‹œì„  ì 
    for gx, gy in gaze_pts:
        cv2.circle(vis, (int(gx), int(gy)), gaze_radius, COLOR_GAZE, -1)

    return vis


import subprocess
import shutil


def convert_avi_to_mp4(avi_path: str) -> str:
    """
    FFmpegë¡œ AVI(MJPG)ë¥¼ MP4(H.264)ë¡œ ë³€í™˜í•œë‹¤.
    avi_path: ì›ë³¸ avi íŒŒì¼ ê²½ë¡œ
    Returns: ë³€í™˜ëœ mp4 íŒŒì¼ ê²½ë¡œ
    """
    if not shutil.which("ffmpeg"):
        raise RuntimeError(
            "FFmpegê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì„œë²„ í™˜ê²½ì— ffmpeg ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        )

    mp4_path = avi_path.replace(".avi", ".mp4")

    cmd = [
        "ffmpeg",
        "-y",
        "-i",
        avi_path,
        "-vcodec",
        "libx264",
        "-preset",
        "fast",
        "-crf",
        "23",
        "-pix_fmt",
        "yuv420p",
        mp4_path,
    ]

    # FFmpeg ì‹¤í–‰
    subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

    return mp4_path


def process_video_once_and_export(
    video_path: str,
    obj_model: YOLO,
    head_model: YOLO,
    sharingan: Sharingan,
    class_meta: Dict[int, Dict[str, str]],
    date: str,
    start: str,
    end: str,
    out_video_path: Optional[str] = None,  # Noneì´ë©´ ì…ë ¥ëª…__vis.mp4
    out_ranking_txt: Optional[str] = None,  # Noneì´ë©´ ./out/<ì…ë ¥ëª…>_ranking.txt
    expand_kx: float = 0.01,
    expand_ky: float = 0.01,
    frame_stride: int = 1,
    max_frames: Optional[int] = None,
    filter_by_inout: bool = False,
    io_thr: float = 0.5,
) -> Tuple[str, str, Dict[int, int], Dict[str, int]]:
    """
    í•œ ë²ˆì˜ íŒ¨ìŠ¤ë¡œ:
      - í”„ë ˆì„ë§ˆë‹¤ obj/head íƒì§€ + gaze ì˜ˆì¸¡
      - ê´€ì‹¬ë„ ëˆ„ì  (ì‹œì„ ì´ í¬í•¨ëœ obj ë°•ìŠ¤ì˜ classì— +1)
      - ì¦‰ì‹œ ì‹œê°í™” í”„ë ˆì„ ì‘ì„±í•˜ì—¬ ë™ì˜ìƒìœ¼ë¡œ ì €ì¥
      - ì¢…ë£Œ í›„ ë­í‚¹ ê³„ì‚°/ì €ì¥

    Returns:
        out_video_path, out_ranking_txt, interest_dict, stats
    """
    # class_names ë§Œë“¤ê¸°
    max_id = max(class_meta.keys()) if class_meta else -1
    class_names = [
        class_meta[i]["name"] if i in class_meta else f"id{i}"
        for i in range(max_id + 1)
    ]

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"Cannot open video: {video_path}")

    W = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    H = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = float(cap.get(cv2.CAP_PROP_FPS) or 30.0)

    # ì¶œë ¥ ê²½ë¡œ ê¸°ë³¸ê°’
    # --- ë‚ ì§œ ê¸°ë°˜ ì¶œë ¥ íŒŒì¼ëª… ìƒì„± ---
    base_name = build_filename(date, start, end)

    # ğŸ”¹ ì¶œë ¥ ì˜ìƒ íŒŒì¼ëª…: 2025-10-25_18:00~19:00.mp4
    out_video_path = str(Path(video_path).with_name(f"{base_name}.avi"))

    # ğŸ”¹ ë­í‚¹ CSV íŒŒì¼ëª…: gaze-tracking_2025-10-25_18:00~19:00.csv
    out_ranking_txt = str(Path("./out") / f"gaze-tracking_{base_name}.csv")

    fourcc_in = int(cap.get(cv2.CAP_PROP_FOURCC))
    writer = cv2.VideoWriter(
        out_video_path, cv2.VideoWriter_fourcc(*"MJPG"), fps, (W, H)
    )

    # ëˆ„ì 
    n_frames_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT) or 0)

    # ì´ ì‹¤í–‰ì—ì„œ ì‹¤ì œë¡œ ì²˜ë¦¬í•  í”„ë ˆì„ ìˆ˜(= stride/ìµœëŒ€í”„ë ˆì„ ë°˜ì˜) ê³„ì‚°
    if n_frames_total > 0:
        frames_cap = (
            n_frames_total if max_frames is None else min(n_frames_total, max_frames)
        )
        total_to_process = (frames_cap + frame_stride - 1) // frame_stride
    else:
        total_to_process = None  # ê¸¸ì´ë¥¼ ëª¨ë¥´ë©´ Noneìœ¼ë¡œ ë‘ê³  ë™ì  ë°”ë¥¼ ì‚¬ìš©

    # tqdm ì§„í–‰ë¥  ë°” ì¤€ë¹„
    pbar = tqdm(
        total=total_to_process,
        desc="Processing",
        unit="f",
        dynamic_ncols=True,
        smoothing=0.1,
    )

    interest: Dict[int, int] = defaultdict(int)
    n_frames = n_frames_total
    n_proc = 0
    n_heads = 0
    n_hits = 0

    processed = 0
    frame_idx = 0

    while True:
        ok, frame_bgr = cap.read()
        if not ok:
            break
        frame_idx += 1
        if frame_stride > 1 and (frame_idx % frame_stride != 1):
            continue

        # (1) ê°ì²´ íƒì§€ + í™•ì¥
        obj_xyxy, obj_cls = run_object_detector_on_frame(
            obj_model, frame_bgr, conf=CONF_THR, iou=IOU_THR, num_classes=NUM_CLASSES
        )
        if obj_xyxy.size > 0:
            obj_xyxy = expand_xyxy_rel_img(obj_xyxy, W, H, kx=expand_kx, ky=expand_ky)

        # (2) ë¨¸ë¦¬/ì‹œì„ 
        gaze_pts, head_xyxy = get_pred_gaze_points_and_heads_from_frame(
            frame_bgr=frame_bgr,
            sharingan=sharingan,
            head_detector=head_model,
            filter_by_inout=filter_by_inout,
            io_thr=io_thr,
        )

        n_proc += 1
        n_heads += int(head_xyxy.shape[0])

        # (3) ê´€ì‹¬ë„ ëˆ„ì 
        if obj_xyxy.size > 0 and gaze_pts.size > 0:
            for pt in gaze_pts:
                idx = point_in_any_box((float(pt[0]), float(pt[1])), obj_xyxy)
                if idx is None:
                    continue
                cid = int(obj_cls[idx])
                if (NUM_CLASSES is None) or (0 <= cid < NUM_CLASSES):
                    interest[cid] += 1
                    n_hits += 1

        # (4) ì‹œê°í™” í”„ë ˆì„ ê¸°ë¡
        vis = draw_annotated_frame(
            frame_bgr=frame_bgr,
            obj_xyxy=obj_xyxy,
            obj_cls=obj_cls,
            head_xyxy=head_xyxy,
            gaze_pts=gaze_pts,
            class_names=class_names,
            thickness_obj=2,
            thickness_head=2,
            gaze_radius=5,
        )
        writer.write(vis)

        # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸  # NEW
        if pbar is not None:
            pbar.update(1)

        processed += 1
        if (max_frames is not None) and (processed >= max_frames):
            break

    cap.release()
    writer.release()

    # (5) ë­í‚¹ ê³„ì‚°/ì €ì¥
    ranked = rank_interest_scores(interest, class_names=class_names, top_k=None)
    # ê¸°ì¡´ write_ranking_txt ìœ í‹¸ ì¬ì‚¬ìš©
    write_ranking_txt(
        out_ranking_txt,
        ranked,
        class_meta=class_meta,
        add_unranked_from_meta=True,
        unranked_sort="id",
    )

    stats = {
        "n_frames": n_frames,
        "n_proc_frames": n_proc,
        "n_heads": n_heads,
        "n_hits": n_hits,
    }
    print(
        f"[DONE] video: {out_video_path} / ranking: {out_ranking_txt} / stats: {stats}",
        file=sys.stderr,
    )
    print(f"[INFO] AVI saved: {out_video_path}", file=sys.stderr)

    # ğŸ”¥ AVI â†’ MP4 ë³€í™˜
    try:
        mp4_path = convert_avi_to_mp4(out_video_path)
        print(f"[INFO] Converted to MP4: {mp4_path}", file=sys.stderr)

        # AVI íŒŒì¼ ì‚­ì œ (ì„ íƒ)
        os.remove(out_video_path)

        # ì—…ë¡œë“œí•  íŒŒì¼ ê²½ë¡œë¥¼ MP4ë¡œ êµì²´
        out_video_path = mp4_path

    except Exception as e:
        print(
            f"[WARN] FFmpeg ë³€í™˜ ì‹¤íŒ¨ â†’ AVI ê·¸ëŒ€ë¡œ ì—…ë¡œë“œí•©ë‹ˆë‹¤: {e}", file=sys.stderr
        )

    print(
        f"[DONE] video: {out_video_path} / ranking: {out_ranking_txt} / stats: {stats}",
        file=sys.stderr,
    )

    return out_video_path, out_ranking_txt, dict(interest), stats


if __name__ == "__main__":
    # í´ë˜ìŠ¤ ì´ë¦„
    CLASS_META = {
        0: {"name": "birak", "category": "beverage", "price": "2000ì›"},
        1: {"name": "sprite", "category": "beverage", "price": "1800ì›"},
        2: {"name": "cornChip", "category": "chip", "price": "1250ì›"},
        3: {"name": "cornChee", "category": "snack", "price": "1500ì›"},
        4: {"name": "hotShrimp", "category": "snack", "price": "1500ì›"},
        5: {"name": "chamCracker", "category": "cracker", "price": "1200ì›"},
    }

    parser = argparse.ArgumentParser(
        description="Run object/head detection + Sharingan gaze on a video once, "
        "save visualization video and ranking .txt in one pass."
    )
    # ê²½ë¡œ ì˜µì…˜
    parser.add_argument("--ckpt", type=str, help="Sharingan checkpoint path")
    parser.add_argument("--obj", type=str, help="YOLO object weights path")
    parser.add_argument("--head", type=str, help="YOLO head weights path")
    parser.add_argument("--video", type=str, help="Input video path")

    parser.add_argument(
        "--out-video", type=str, default=None, help="Output visualization video path"
    )
    parser.add_argument(
        "--out-ranking", type=str, default=None, help="Output ranking .txt path"
    )
    parser.add_argument(
        "--expand-kx",
        type=float,
        default=0.01,
        help="Object box expand ratio (relative to width)",
    )
    parser.add_argument(
        "--expand-ky",
        type=float,
        default=0.01,
        help="Object box expand ratio (relative to height)",
    )
    parser.add_argument(
        "--frame-stride", type=int, default=1, help="Sample every Nth frame"
    )
    parser.add_argument(
        "--max-frames", type=int, default=None, help="Limit number of processed frames"
    )
    parser.add_argument(
        "--filter-inout",
        action="store_true",
        default=False,
        help="Filter gaze by in/out score",
    )
    parser.add_argument(
        "--io-thr", type=float, default=0.5, help="Threshold for in/out filtering"
    )

    # ë‚ ì§œÂ·ì‹œê°„ ê¸°ë°˜ ì¶œë ¥ íŒŒì¼ëª… ìƒì„±ìš©
    parser.add_argument("--date", type=str, required=True, help="Date (YYYY-MM-DD)")
    parser.add_argument("--start", type=str, required=True, help="Start time (HH:MM)")
    parser.add_argument("--end", type=str, required=True, help="End time (HH:MM)")

    args = parser.parse_args()

    # args.ckpt = "checkpoints/best_gaze_synth_experiment_3.ckpt"
    # args.obj = "weights/detect_experiment_object.pt"
    # args.head = "weights/experiment_head.pt"
    # args.video = "./data/20250630_164710.mp4"

    # 3) ëª¨ë¸ ë¡œë“œ
    obj_model = load_obj_detection_model(DEVICE, weights_path=args.obj)
    head_model = load_head_detection_model(DEVICE, weights_path=args.head)
    sharingan = load_sharingan_model(args.ckpt, DEVICE)

    # 4) ë¹„ë””ì˜¤ ì²˜ë¦¬
    out_vid, out_txt, interest, stats = process_video_once_and_export(
        video_path=args.video,
        obj_model=obj_model,
        head_model=head_model,
        sharingan=sharingan,
        class_meta=CLASS_META,
        date=args.date,
        start=args.start,
        end=args.end,
        out_video_path=args.out_video,  # Noneì´ë©´ ìë™ ê²½ë¡œ(ì…ë ¥ëª…__vis.mp4)
        out_ranking_txt=args.out_ranking,  # Noneì´ë©´ ./out/<ì…ë ¥ëª…>_ranking.txt
        expand_kx=args.expand_kx,
        expand_ky=args.expand_ky,
        frame_stride=args.frame_stride,
        max_frames=args.max_frames,
        filter_by_inout=args.filter_inout,
        io_thr=args.io_thr,
    )

    # --- JSON ê²°ê³¼ ì¶œë ¥ (FastAPIìš©) ---
    # --- Google Drive ì—…ë¡œë“œ --- #
    try:
        folder_id = "1ZRAfqwSe7vnxMqN6rlu9KcJxTmMvxMBz"
        video_link, video_view = upload_to_drive(
            out_vid, "video/mp4", folder_id=folder_id
        )
        txt_link, txt_view = upload_to_drive(out_txt, "text/csv", folder_id=folder_id)

        # ë¡œì»¬ íŒŒì¼ ì •ë¦¬
        os.remove(out_vid)
        os.remove(out_txt)

    except Exception as e:
        video_link = None
        txt_link = None
        print(f"[Drive Upload Error] {e}", file=sys.stderr)

    # --- JSON ê²°ê³¼ ì¶œë ¥ (FastAPIìš©) --- #
    import json

    result = {
        "video_drive_link": video_link,
        "ranking_drive_link": txt_link,
        "interest": interest,
        "stats": stats,
    }

    sys.stdout.write(json.dumps(result, default=str))
    sys.stdout.flush()
    print(f"[UPLOAD] Video uploaded to: {video_link}", file=sys.stderr)
    print(f"[UPLOAD] Ranking uploaded to: {txt_link}", file=sys.stderr)
